{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2a4cde",
   "metadata": {},
   "source": [
    "# Train MLP Baseline (toy example and hyperparameter tuning)\n",
    "\n",
    "This notebook trains the `MLPBaseline` model from `model_classes/mlp_baseline.py` with a simple forward/backward loop and a small hyperparameter grid search.\n",
    "\n",
    "Notes:\n",
    "- For demonstration this uses a synthetic dataset (random features + labels). Replace the data-loading cell with your real dataset / `DataLoader` from `data_loaders.py` when ready.\n",
    "- Training loop is minimal: forward, compute loss, backward, optimizer step.\n",
    "- Hyperparameter tuning is a simple grid search over a few combos; it's not parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14fe8cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root in sys.path: /home/quiet98k/Code/Fall25/hand-gestures-classifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming you start the notebook from the training_scripts/ directory\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(\"Project root in sys.path:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e87cb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Basic imports and helper utilities\n",
    "import os\n",
    "from itertools import product\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import model factory and dataset from the repo\n",
    "from model_classes.mlp_baseline import create_mlp_baseline\n",
    "from data_loaders import LandmarksDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf33e630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: train= 408237 val= 53392\n",
      "Input dim (flattened landmarks): 42 Num classes: 18\n",
      "Successfully retrieved a batch from train_loader\n",
      "Batch shapes: torch.Size([64, 42]) torch.Size([64])\n",
      "Successfully retrieved a batch from train_loader\n",
      "Batch shapes: torch.Size([64, 42]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Use the real LandmarksDataset from the repository\n",
    "# We flatten landmarks for the MLP (shape: N_landmarks * 2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create train/val dataset instances (assumes data/annotations/{train,val} exist)\n",
    "annotations_root = \"../data/annotations\"\n",
    "train_ds = LandmarksDataset(annotations_root=annotations_root, split='train', flatten=True)\n",
    "val_ds = LandmarksDataset(annotations_root=annotations_root, split='val', flatten=True)\n",
    "\n",
    "# Infer input_dim and num_classes from dataset (keeps things consistent)\n",
    "first = train_ds[0]['landmarks']\n",
    "if isinstance(first, torch.Tensor):\n",
    "    input_dim = int(first.numel())\n",
    "else:\n",
    "    # Fallback: assume 42 features if something unusual happens\n",
    "    input_dim = 42\n",
    "num_classes = train_ds.num_classes\n",
    "\n",
    "print('Dataset sizes: train=', len(train_ds), 'val=', len(val_ds))\n",
    "print('Input dim (flattened landmarks):', input_dim, 'Num classes:', num_classes)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, drop_last=False)\n",
    "\n",
    "# Quick smoke test: retrieve one batch and print basic shapes\n",
    "try:\n",
    "    batch = next(iter(train_loader))\n",
    "    print('Successfully retrieved a batch from train_loader')\n",
    "except Exception as e:\n",
    "    print('Error when iterating train_loader:', repr(e))\n",
    "    raise\n",
    "\n",
    "# Extract landmarks and labels from batch\n",
    "if isinstance(batch, dict):\n",
    "    xb = batch['landmarks']\n",
    "    yb = batch['label']\n",
    "else:\n",
    "    xb, yb = batch\n",
    "\n",
    "print('Batch shapes:', getattr(xb, 'shape', None), getattr(yb, 'shape', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b88cf0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation helpers\n",
    "def train_epoch(model: nn.Module, loader: DataLoader, optimizer, criterion) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        # Support both dict-style batches (from our Dataset) and tuple batches\n",
    "        if isinstance(batch, dict):\n",
    "            xb = batch['landmarks']\n",
    "            yb = batch['label']\n",
    "        else:\n",
    "            xb, yb = batch\n",
    "\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total,\n",
    "        'acc': total_correct / total,\n",
    "    }\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if isinstance(batch, dict):\n",
    "                xb = batch['landmarks']\n",
    "                yb = batch['label']\n",
    "            else:\n",
    "                xb, yb = batch\n",
    "\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total,\n",
    "        'acc': total_correct / total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ece05e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running config: {'lr': 0.001, 'hidden_dims': (128, 64), 'dropout': 0.1, 'batchnorm': False, 'batch_size': 32, 'max_epochs': 8}\n",
      "Epoch 1/8  train_loss=0.3068 train_acc=0.8973  val_loss=0.0724 val_acc=0.9818\n",
      "Epoch 1/8  train_loss=0.3068 train_acc=0.8973  val_loss=0.0724 val_acc=0.9818\n",
      "Epoch 2/8  train_loss=0.0881 train_acc=0.9761  val_loss=0.0523 val_acc=0.9867\n",
      "Epoch 2/8  train_loss=0.0881 train_acc=0.9761  val_loss=0.0523 val_acc=0.9867\n",
      "Epoch 3/8  train_loss=0.0740 train_acc=0.9798  val_loss=0.0530 val_acc=0.9857\n",
      "Epoch 3/8  train_loss=0.0740 train_acc=0.9798  val_loss=0.0530 val_acc=0.9857\n",
      "Epoch 4/8  train_loss=0.0670 train_acc=0.9815  val_loss=0.0455 val_acc=0.9879\n",
      "Epoch 4/8  train_loss=0.0670 train_acc=0.9815  val_loss=0.0455 val_acc=0.9879\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m best_state: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 38\u001b[0m     tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     va \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  train_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m train_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     23\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m xb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     24\u001b[0m     preds \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: total_loss \u001b[38;5;241m/\u001b[39m total,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m: total_correct \u001b[38;5;241m/\u001b[39m total,\n\u001b[1;32m     31\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Simple hyperparameter grid search (small grid for demo)\n",
    "results = []\n",
    "save_dir = os.path.join(os.getcwd(), 'training_outputs')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 5e-4],\n",
    "    'hidden_dims': [(128, 64), (256, 128)],\n",
    "    'dropout': [0.1, 0.2],\n",
    "    'batchnorm': [False, True],\n",
    "    # tune batch size and number of epochs as well\n",
    "    'batch_size': [32, 64],\n",
    "    'max_epochs': [8, 12],\n",
    "}\n",
    "\n",
    "# Loss (kept separate for clarity)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# iterate grid\n",
    "for lr, hidden_dims, dropout, batchnorm, batch_size, max_epochs in product(\n",
    "    param_grid['lr'], param_grid['hidden_dims'], param_grid['dropout'], param_grid['batchnorm'],\n",
    "    param_grid['batch_size'], param_grid['max_epochs']\n",
    "):\n",
    "    print('Running config:', {'lr': lr, 'hidden_dims': hidden_dims, 'dropout': dropout, 'batchnorm': batchnorm, 'batch_size': batch_size, 'max_epochs': max_epochs})\n",
    "\n",
    "    # build model and loaders\n",
    "    model = create_mlp_baseline(input_dim=input_dim, num_classes=num_classes, hidden_dims=hidden_dims, dropout=dropout, activation='relu', batchnorm=batchnorm)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # create DataLoaders directly (no helper)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state: dict[str, Any] | None = None\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        tr = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        va = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch}/{max_epochs}  train_loss={tr['loss']:.4f} train_acc={tr['acc']:.4f}  val_loss={va['loss']:.4f} val_acc={va['acc']:.4f}\")\n",
    "\n",
    "        # save best model for this config\n",
    "        if va['acc'] > best_val_acc:\n",
    "            best_val_acc = va['acc']\n",
    "            best_state = {\n",
    "                'model_state': model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "\n",
    "    # persist best for this config\n",
    "    config_name = f\"mlp_lr{lr}_hd{hidden_dims[0]}-{hidden_dims[1]}_do{int(dropout*100)}_bn{int(batchnorm)}_bs{batch_size}_ep{max_epochs}\"\n",
    "    out_path = os.path.join(save_dir, config_name + '.pth')\n",
    "    if best_state is not None:\n",
    "        torch.save({'config': {'lr': lr, 'hidden_dims': hidden_dims, 'dropout': dropout, 'batchnorm': batchnorm, 'batch_size': batch_size, 'max_epochs': max_epochs}, 'best_val_acc': best_val_acc, 'state': best_state}, out_path)\n",
    "\n",
    "    results.append({'config': {'lr': lr, 'hidden_dims': hidden_dims, 'dropout': dropout, 'batchnorm': batchnorm, 'batch_size': batch_size, 'max_epochs': max_epochs}, 'best_val_acc': best_val_acc, 'path': out_path})\n",
    "\n",
    "# summarize results\n",
    "results_sorted = sorted(results, key=lambda r: r['best_val_acc'], reverse=True)\n",
    "print('Top results:')\n",
    "for r in results_sorted[:5]:\n",
    "    print(r['best_val_acc'], r['config'], r['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and do a quick inference check\n",
    "best = results_sorted[0]\n",
    "print('Best config:', best['config'], 'val_acc=', best['best_val_acc'])\n",
    "ckpt = torch.load(best['path'], map_location=device)\n",
    "cfg = ckpt['config']\n",
    "model = create_mlp_baseline(input_dim=input_dim, num_classes=num_classes, hidden_dims=tuple(cfg['hidden_dims']), dropout=cfg['dropout'], activation='relu', batchnorm=cfg['batchnorm'])\n",
    "model.load_state_dict(ckpt['state']['model_state'])\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# run inference on a small batch\n",
    "xb, yb = next(iter(val_loader))\n",
    "with torch.no_grad():\n",
    "    logits = model(xb.to(device))\n",
    "    preds = logits.argmax(dim=1).cpu()\n",
    "\n",
    "print('Sample preds:', preds[:10].tolist())\n",
    "print('Sample labels:', yb[:10].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hand-gestures-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
