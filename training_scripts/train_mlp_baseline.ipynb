{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2a4cde",
   "metadata": {},
   "source": [
    "# Train MLP Baseline (toy example and hyperparameter tuning)\n",
    "\n",
    "This notebook trains the `MLPBaseline` model from `model_classes/mlp_baseline.py` with a simple forward/backward loop and a small hyperparameter grid search.\n",
    "\n",
    "Notes:\n",
    "- For demonstration this uses a synthetic dataset (random features + labels). Replace the data-loading cell with your real dataset / `DataLoader` from `data_loaders.py` when ready.\n",
    "- Training loop is minimal: forward, compute loss, backward, optimizer step.\n",
    "- Hyperparameter tuning is a simple grid search over a few combos; it's not parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fe8cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root in sys.path: /home/quiet98k/Code/Fall25/hand-gestures-classifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming you start the notebook from the training_scripts/ directory\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(\"Project root in sys.path:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e87cb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Basic imports and helper utilities\n",
    "import os\n",
    "from itertools import product\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import model factory and dataset from the repo\n",
    "from model_classes.mlp_baseline import create_mlp_baseline\n",
    "from data_loaders import LandmarksDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf33e630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: train= 408237 val= 53392\n",
      "Input dim (flattened landmarks): 42 Num classes: 18\n",
      "Successfully retrieved a batch from train_loader\n",
      "Batch shapes: torch.Size([64, 42]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Use the real LandmarksDataset from the repository\n",
    "# We flatten landmarks for the MLP (shape: N_landmarks * 2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create train/val dataset instances (assumes data/annotations/{train,val} exist)\n",
    "annotations_root = \"../data/annotations\"\n",
    "train_ds = LandmarksDataset(annotations_root=annotations_root, split='train', flatten=True)\n",
    "val_ds = LandmarksDataset(annotations_root=annotations_root, split='val', flatten=True)\n",
    "\n",
    "# Infer input_dim and num_classes from dataset (keeps things consistent)\n",
    "first = train_ds[0]['landmarks']\n",
    "if isinstance(first, torch.Tensor):\n",
    "    input_dim = int(first.numel())\n",
    "else:\n",
    "    # Fallback: assume 42 features if something unusual happens\n",
    "    input_dim = 42\n",
    "num_classes = train_ds.num_classes\n",
    "\n",
    "print('Dataset sizes: train=', len(train_ds), 'val=', len(val_ds))\n",
    "print('Input dim (flattened landmarks):', input_dim, 'Num classes:', num_classes)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, drop_last=False)\n",
    "\n",
    "# Quick smoke test: retrieve one batch and print basic shapes\n",
    "try:\n",
    "    batch = next(iter(train_loader))\n",
    "    print('Successfully retrieved a batch from train_loader')\n",
    "except Exception as e:\n",
    "    print('Error when iterating train_loader:', repr(e))\n",
    "    raise\n",
    "\n",
    "# Extract landmarks and labels from batch\n",
    "if isinstance(batch, dict):\n",
    "    xb = batch['landmarks']\n",
    "    yb = batch['label']\n",
    "else:\n",
    "    xb, yb = batch\n",
    "\n",
    "print('Batch shapes:', getattr(xb, 'shape', None), getattr(yb, 'shape', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b88cf0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation helpers\n",
    "def train_epoch(model: nn.Module, loader: DataLoader, optimizer, criterion, log_interval: int = 100) -> Dict[str, float]:\n",
    "    \"\"\"Run one training epoch and optionally print batch-level summaries.\n",
    "    Prints periodic batch summaries showing batch index, remaining batches, avg loss and acc for the window.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    window_loss = 0.0\n",
    "    window_correct = 0\n",
    "    window_samples = 0\n",
    "    for batch_idx, batch in enumerate(loader, start=1):\n",
    "        # Support both dict-style batches (from our Dataset) and tuple batches\n",
    "        if isinstance(batch, dict):\n",
    "            xb = batch['landmarks']\n",
    "            yb = batch['label']\n",
    "        else:\n",
    "            xb, yb = batch\n",
    "\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bsz = xb.size(0)\n",
    "        total_loss += loss.item() * bsz\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct = (preds == yb).sum().item()\n",
    "        total_correct += correct\n",
    "        total += bsz\n",
    "\n",
    "        # update window\n",
    "        window_loss += loss.item() * bsz\n",
    "        window_correct += correct\n",
    "        window_samples += bsz\n",
    "\n",
    "        # periodic print every `log_interval` batches\n",
    "        if log_interval and (batch_idx % log_interval == 0):\n",
    "            avg_loss = window_loss / window_samples if window_samples else 0.0\n",
    "            avg_acc = window_correct / window_samples if window_samples else 0.0\n",
    "            total_batches = len(loader) if hasattr(loader, '__len__') else '??'\n",
    "            print(f\"Batch {batch_idx}/{total_batches} | avg_loss={avg_loss:.4f} avg_acc={avg_acc:.4f}\")\n",
    "            # reset window counters\n",
    "            window_loss = 0.0\n",
    "            window_correct = 0\n",
    "            window_samples = 0\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total,\n",
    "        'acc': total_correct / total,\n",
    "    }\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if isinstance(batch, dict):\n",
    "                xb = batch['landmarks']\n",
    "                yb = batch['label']\n",
    "            else:\n",
    "                xb, yb = batch\n",
    "\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == yb).sum().item()\n",
    "            total += xb.size(0)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / total,\n",
    "        'acc': total_correct / total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece05e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running config: {'lr': 0.0003, 'hidden_dims': (128, 64), 'dropout': 0.1, 'batchnorm': False, 'batch_size': 128, 'max_epochs': 8}\n",
      "Batch 500/3190 | avg_loss=1.9988 avg_acc=0.3665\n",
      "Batch 500/3190 | avg_loss=1.9988 avg_acc=0.3665\n",
      "Batch 1000/3190 | avg_loss=1.1221 avg_acc=0.5476\n",
      "Batch 1000/3190 | avg_loss=1.1221 avg_acc=0.5476\n",
      "Batch 1500/3190 | avg_loss=0.9480 avg_acc=0.6249\n",
      "Batch 1500/3190 | avg_loss=0.9480 avg_acc=0.6249\n",
      "Batch 2000/3190 | avg_loss=0.7745 avg_acc=0.7134\n",
      "Batch 2000/3190 | avg_loss=0.7745 avg_acc=0.7134\n",
      "Batch 2500/3190 | avg_loss=0.6105 avg_acc=0.7880\n",
      "Batch 2500/3190 | avg_loss=0.6105 avg_acc=0.7880\n",
      "Batch 3000/3190 | avg_loss=0.4943 avg_acc=0.8326\n",
      "Batch 3000/3190 | avg_loss=0.4943 avg_acc=0.8326\n",
      "Epoch 1/8 | train_loss=0.9588 train_acc=0.6578 | val_loss=0.3466 val_acc=0.9184\n",
      "Epoch 1/8 | train_loss=0.9588 train_acc=0.6578 | val_loss=0.3466 val_acc=0.9184\n",
      "Batch 500/3190 | avg_loss=0.4028 avg_acc=0.8762\n",
      "Batch 500/3190 | avg_loss=0.4028 avg_acc=0.8762\n",
      "Batch 1000/3190 | avg_loss=0.3523 avg_acc=0.8911\n",
      "Batch 1000/3190 | avg_loss=0.3523 avg_acc=0.8911\n",
      "Batch 1500/3190 | avg_loss=0.3115 avg_acc=0.9064\n",
      "Batch 1500/3190 | avg_loss=0.3115 avg_acc=0.9064\n",
      "Batch 2000/3190 | avg_loss=0.2791 avg_acc=0.9201\n",
      "Batch 2000/3190 | avg_loss=0.2791 avg_acc=0.9201\n",
      "Batch 2500/3190 | avg_loss=0.2507 avg_acc=0.9280\n",
      "Batch 2500/3190 | avg_loss=0.2507 avg_acc=0.9280\n",
      "Batch 3000/3190 | avg_loss=0.2206 avg_acc=0.9392\n",
      "Batch 3000/3190 | avg_loss=0.2206 avg_acc=0.9392\n",
      "Epoch 2/8 | train_loss=0.2980 train_acc=0.9119 | val_loss=0.1429 val_acc=0.9685\n",
      "Epoch 2/8 | train_loss=0.2980 train_acc=0.9119 | val_loss=0.1429 val_acc=0.9685\n",
      "Batch 500/3190 | avg_loss=0.1964 avg_acc=0.9475\n",
      "Batch 500/3190 | avg_loss=0.1964 avg_acc=0.9475\n",
      "Batch 1000/3190 | avg_loss=0.1888 avg_acc=0.9509\n",
      "Batch 1000/3190 | avg_loss=0.1888 avg_acc=0.9509\n",
      "Batch 1500/3190 | avg_loss=0.1708 avg_acc=0.9541\n",
      "Batch 1500/3190 | avg_loss=0.1708 avg_acc=0.9541\n",
      "Batch 2000/3190 | avg_loss=0.1609 avg_acc=0.9582\n",
      "Batch 2000/3190 | avg_loss=0.1609 avg_acc=0.9582\n",
      "Batch 2500/3190 | avg_loss=0.1548 avg_acc=0.9601\n",
      "Batch 2500/3190 | avg_loss=0.1548 avg_acc=0.9601\n",
      "Batch 3000/3190 | avg_loss=0.1443 avg_acc=0.9632\n",
      "Batch 3000/3190 | avg_loss=0.1443 avg_acc=0.9632\n",
      "Epoch 3/8 | train_loss=0.1678 train_acc=0.9561 | val_loss=0.0943 val_acc=0.9780\n",
      "Epoch 3/8 | train_loss=0.1678 train_acc=0.9561 | val_loss=0.0943 val_acc=0.9780\n",
      "Batch 500/3190 | avg_loss=0.1388 avg_acc=0.9637\n",
      "Batch 500/3190 | avg_loss=0.1388 avg_acc=0.9637\n",
      "Batch 1000/3190 | avg_loss=0.1327 avg_acc=0.9662\n",
      "Batch 1000/3190 | avg_loss=0.1327 avg_acc=0.9662\n",
      "Batch 1500/3190 | avg_loss=0.1296 avg_acc=0.9663\n",
      "Batch 1500/3190 | avg_loss=0.1296 avg_acc=0.9663\n",
      "Batch 2000/3190 | avg_loss=0.1233 avg_acc=0.9672\n",
      "Batch 2000/3190 | avg_loss=0.1233 avg_acc=0.9672\n",
      "Batch 2500/3190 | avg_loss=0.1214 avg_acc=0.9703\n",
      "Batch 2500/3190 | avg_loss=0.1214 avg_acc=0.9703\n",
      "Batch 3000/3190 | avg_loss=0.1155 avg_acc=0.9705\n",
      "Batch 3000/3190 | avg_loss=0.1155 avg_acc=0.9705\n",
      "Epoch 4/8 | train_loss=0.1261 train_acc=0.9676 | val_loss=0.0784 val_acc=0.9814\n",
      "Epoch 4/8 | train_loss=0.1261 train_acc=0.9676 | val_loss=0.0784 val_acc=0.9814\n",
      "Batch 500/3190 | avg_loss=0.1127 avg_acc=0.9716\n",
      "Batch 500/3190 | avg_loss=0.1127 avg_acc=0.9716\n",
      "Batch 1000/3190 | avg_loss=0.1145 avg_acc=0.9707\n",
      "Batch 1000/3190 | avg_loss=0.1145 avg_acc=0.9707\n",
      "Batch 1500/3190 | avg_loss=0.1093 avg_acc=0.9722\n",
      "Batch 1500/3190 | avg_loss=0.1093 avg_acc=0.9722\n",
      "Batch 2000/3190 | avg_loss=0.0986 avg_acc=0.9751\n",
      "Batch 2000/3190 | avg_loss=0.0986 avg_acc=0.9751\n",
      "Batch 2500/3190 | avg_loss=0.1047 avg_acc=0.9725\n",
      "Batch 2500/3190 | avg_loss=0.1047 avg_acc=0.9725\n",
      "Batch 3000/3190 | avg_loss=0.0973 avg_acc=0.9750\n",
      "Batch 3000/3190 | avg_loss=0.0973 avg_acc=0.9750\n",
      "Epoch 5/8 | train_loss=0.1058 train_acc=0.9730 | val_loss=0.0673 val_acc=0.9837\n",
      "Epoch 5/8 | train_loss=0.1058 train_acc=0.9730 | val_loss=0.0673 val_acc=0.9837\n",
      "Batch 500/3190 | avg_loss=0.0981 avg_acc=0.9756\n",
      "Batch 500/3190 | avg_loss=0.0981 avg_acc=0.9756\n",
      "Batch 1000/3190 | avg_loss=0.0949 avg_acc=0.9752\n",
      "Batch 1000/3190 | avg_loss=0.0949 avg_acc=0.9752\n",
      "Batch 1500/3190 | avg_loss=0.0962 avg_acc=0.9757\n",
      "Batch 1500/3190 | avg_loss=0.0962 avg_acc=0.9757\n",
      "Batch 2000/3190 | avg_loss=0.0948 avg_acc=0.9758\n",
      "Batch 2000/3190 | avg_loss=0.0948 avg_acc=0.9758\n",
      "Batch 2500/3190 | avg_loss=0.0932 avg_acc=0.9774\n",
      "Batch 2500/3190 | avg_loss=0.0932 avg_acc=0.9774\n",
      "Batch 3000/3190 | avg_loss=0.0881 avg_acc=0.9773\n",
      "Batch 3000/3190 | avg_loss=0.0881 avg_acc=0.9773\n",
      "Epoch 6/8 | train_loss=0.0936 train_acc=0.9763 | val_loss=0.0632 val_acc=0.9843\n",
      "Epoch 6/8 | train_loss=0.0936 train_acc=0.9763 | val_loss=0.0632 val_acc=0.9843\n",
      "Batch 500/3190 | avg_loss=0.0882 avg_acc=0.9771\n",
      "Batch 500/3190 | avg_loss=0.0882 avg_acc=0.9771\n",
      "Batch 1000/3190 | avg_loss=0.0861 avg_acc=0.9778\n",
      "Batch 1000/3190 | avg_loss=0.0861 avg_acc=0.9778\n",
      "Batch 1500/3190 | avg_loss=0.0827 avg_acc=0.9784\n",
      "Batch 1500/3190 | avg_loss=0.0827 avg_acc=0.9784\n",
      "Batch 2000/3190 | avg_loss=0.0838 avg_acc=0.9786\n",
      "Batch 2000/3190 | avg_loss=0.0838 avg_acc=0.9786\n",
      "Batch 2500/3190 | avg_loss=0.0819 avg_acc=0.9792\n",
      "Batch 2500/3190 | avg_loss=0.0819 avg_acc=0.9792\n",
      "Batch 3000/3190 | avg_loss=0.0818 avg_acc=0.9794\n",
      "Batch 3000/3190 | avg_loss=0.0818 avg_acc=0.9794\n",
      "Epoch 7/8 | train_loss=0.0837 train_acc=0.9786 | val_loss=0.0585 val_acc=0.9857\n",
      "Epoch 7/8 | train_loss=0.0837 train_acc=0.9786 | val_loss=0.0585 val_acc=0.9857\n",
      "Batch 500/3190 | avg_loss=0.0818 avg_acc=0.9795\n",
      "Batch 500/3190 | avg_loss=0.0818 avg_acc=0.9795\n",
      "Batch 1000/3190 | avg_loss=0.0766 avg_acc=0.9804\n",
      "Batch 1000/3190 | avg_loss=0.0766 avg_acc=0.9804\n",
      "Batch 1500/3190 | avg_loss=0.0772 avg_acc=0.9803\n",
      "Batch 1500/3190 | avg_loss=0.0772 avg_acc=0.9803\n",
      "Batch 2000/3190 | avg_loss=0.0769 avg_acc=0.9796\n",
      "Batch 2000/3190 | avg_loss=0.0769 avg_acc=0.9796\n",
      "Batch 2500/3190 | avg_loss=0.0753 avg_acc=0.9808\n",
      "Batch 2500/3190 | avg_loss=0.0753 avg_acc=0.9808\n",
      "Batch 3000/3190 | avg_loss=0.0754 avg_acc=0.9811\n",
      "Batch 3000/3190 | avg_loss=0.0754 avg_acc=0.9811\n",
      "Epoch 8/8 | train_loss=0.0773 train_acc=0.9803 | val_loss=0.0520 val_acc=0.9872\n",
      "Running config: {'lr': 0.0003, 'hidden_dims': (128, 64), 'dropout': 0.1, 'batchnorm': False, 'batch_size': 128, 'max_epochs': 12}\n",
      "Epoch 8/8 | train_loss=0.0773 train_acc=0.9803 | val_loss=0.0520 val_acc=0.9872\n",
      "Running config: {'lr': 0.0003, 'hidden_dims': (128, 64), 'dropout': 0.1, 'batchnorm': False, 'batch_size': 128, 'max_epochs': 12}\n",
      "Batch 500/3190 | avg_loss=2.0111 avg_acc=0.3728\n",
      "Batch 500/3190 | avg_loss=2.0111 avg_acc=0.3728\n",
      "Batch 1000/3190 | avg_loss=1.1010 avg_acc=0.5748\n",
      "Batch 1000/3190 | avg_loss=1.1010 avg_acc=0.5748\n",
      "Batch 1500/3190 | avg_loss=0.8853 avg_acc=0.6736\n",
      "Batch 1500/3190 | avg_loss=0.8853 avg_acc=0.6736\n",
      "Batch 2000/3190 | avg_loss=0.6702 avg_acc=0.7719\n",
      "Batch 2000/3190 | avg_loss=0.6702 avg_acc=0.7719\n",
      "Batch 2500/3190 | avg_loss=0.5169 avg_acc=0.8245\n",
      "Batch 2500/3190 | avg_loss=0.5169 avg_acc=0.8245\n",
      "Batch 3000/3190 | avg_loss=0.4275 avg_acc=0.8597\n",
      "Batch 3000/3190 | avg_loss=0.4275 avg_acc=0.8597\n",
      "Epoch 1/12 | train_loss=0.9030 train_acc=0.6913 | val_loss=0.2941 val_acc=0.9288\n",
      "Epoch 1/12 | train_loss=0.9030 train_acc=0.6913 | val_loss=0.2941 val_acc=0.9288\n",
      "Batch 500/3190 | avg_loss=0.3381 avg_acc=0.8948\n",
      "Batch 500/3190 | avg_loss=0.3381 avg_acc=0.8948\n",
      "Batch 1000/3190 | avg_loss=0.3050 avg_acc=0.9095\n",
      "Batch 1000/3190 | avg_loss=0.3050 avg_acc=0.9095\n",
      "Batch 1500/3190 | avg_loss=0.2641 avg_acc=0.9256\n",
      "Batch 1500/3190 | avg_loss=0.2641 avg_acc=0.9256\n",
      "Batch 2000/3190 | avg_loss=0.2287 avg_acc=0.9381\n",
      "Batch 2000/3190 | avg_loss=0.2287 avg_acc=0.9381\n",
      "Batch 2500/3190 | avg_loss=0.2056 avg_acc=0.9452\n",
      "Batch 2500/3190 | avg_loss=0.2056 avg_acc=0.9452\n",
      "Batch 3000/3190 | avg_loss=0.1860 avg_acc=0.9509\n",
      "Batch 3000/3190 | avg_loss=0.1860 avg_acc=0.9509\n",
      "Epoch 2/12 | train_loss=0.2499 train_acc=0.9289 | val_loss=0.1177 val_acc=0.9732\n",
      "Epoch 2/12 | train_loss=0.2499 train_acc=0.9289 | val_loss=0.1177 val_acc=0.9732\n",
      "Batch 500/3190 | avg_loss=0.1700 avg_acc=0.9559\n",
      "Batch 500/3190 | avg_loss=0.1700 avg_acc=0.9559\n",
      "Batch 1000/3190 | avg_loss=0.1574 avg_acc=0.9597\n",
      "Batch 1000/3190 | avg_loss=0.1574 avg_acc=0.9597\n",
      "Batch 1500/3190 | avg_loss=0.1473 avg_acc=0.9617\n",
      "Batch 1500/3190 | avg_loss=0.1473 avg_acc=0.9617\n",
      "Batch 2000/3190 | avg_loss=0.1375 avg_acc=0.9648\n",
      "Batch 2000/3190 | avg_loss=0.1375 avg_acc=0.9648\n",
      "Batch 2500/3190 | avg_loss=0.1332 avg_acc=0.9661\n",
      "Batch 2500/3190 | avg_loss=0.1332 avg_acc=0.9661\n",
      "Batch 3000/3190 | avg_loss=0.1256 avg_acc=0.9679\n",
      "Batch 3000/3190 | avg_loss=0.1256 avg_acc=0.9679\n",
      "Epoch 3/12 | train_loss=0.1434 train_acc=0.9632 | val_loss=0.0852 val_acc=0.9792\n",
      "Epoch 3/12 | train_loss=0.1434 train_acc=0.9632 | val_loss=0.0852 val_acc=0.9792\n",
      "Batch 500/3190 | avg_loss=0.1176 avg_acc=0.9701\n",
      "Batch 500/3190 | avg_loss=0.1176 avg_acc=0.9701\n",
      "Batch 1000/3190 | avg_loss=0.1170 avg_acc=0.9707\n",
      "Batch 1000/3190 | avg_loss=0.1170 avg_acc=0.9707\n",
      "Batch 1500/3190 | avg_loss=0.1115 avg_acc=0.9715\n",
      "Batch 1500/3190 | avg_loss=0.1115 avg_acc=0.9715\n",
      "Batch 2000/3190 | avg_loss=0.1081 avg_acc=0.9725\n",
      "Batch 2000/3190 | avg_loss=0.1081 avg_acc=0.9725\n",
      "Batch 2500/3190 | avg_loss=0.1045 avg_acc=0.9732\n",
      "Batch 2500/3190 | avg_loss=0.1045 avg_acc=0.9732\n",
      "Batch 3000/3190 | avg_loss=0.1033 avg_acc=0.9737\n",
      "Batch 3000/3190 | avg_loss=0.1033 avg_acc=0.9737\n",
      "Epoch 4/12 | train_loss=0.1096 train_acc=0.9720 | val_loss=0.0690 val_acc=0.9827\n",
      "Epoch 4/12 | train_loss=0.1096 train_acc=0.9720 | val_loss=0.0690 val_acc=0.9827\n",
      "Batch 500/3190 | avg_loss=0.0993 avg_acc=0.9746\n",
      "Batch 500/3190 | avg_loss=0.0993 avg_acc=0.9746\n",
      "Batch 1000/3190 | avg_loss=0.0983 avg_acc=0.9745\n",
      "Batch 1000/3190 | avg_loss=0.0983 avg_acc=0.9745\n",
      "Batch 1500/3190 | avg_loss=0.0987 avg_acc=0.9753\n",
      "Batch 1500/3190 | avg_loss=0.0987 avg_acc=0.9753\n",
      "Batch 2000/3190 | avg_loss=0.0893 avg_acc=0.9768\n",
      "Batch 2000/3190 | avg_loss=0.0893 avg_acc=0.9768\n",
      "Batch 2500/3190 | avg_loss=0.0879 avg_acc=0.9777\n",
      "Batch 2500/3190 | avg_loss=0.0879 avg_acc=0.9777\n",
      "Batch 3000/3190 | avg_loss=0.0921 avg_acc=0.9768\n",
      "Batch 3000/3190 | avg_loss=0.0921 avg_acc=0.9768\n",
      "Epoch 5/12 | train_loss=0.0937 train_acc=0.9761 | val_loss=0.0598 val_acc=0.9847\n",
      "Epoch 5/12 | train_loss=0.0937 train_acc=0.9761 | val_loss=0.0598 val_acc=0.9847\n",
      "Batch 500/3190 | avg_loss=0.0849 avg_acc=0.9775\n",
      "Batch 500/3190 | avg_loss=0.0849 avg_acc=0.9775\n",
      "Batch 1000/3190 | avg_loss=0.0869 avg_acc=0.9780\n",
      "Batch 1000/3190 | avg_loss=0.0869 avg_acc=0.9780\n",
      "Batch 1500/3190 | avg_loss=0.0845 avg_acc=0.9782\n",
      "Batch 1500/3190 | avg_loss=0.0845 avg_acc=0.9782\n",
      "Batch 2000/3190 | avg_loss=0.0828 avg_acc=0.9793\n",
      "Batch 2000/3190 | avg_loss=0.0828 avg_acc=0.9793\n",
      "Batch 2500/3190 | avg_loss=0.0769 avg_acc=0.9801\n",
      "Batch 2500/3190 | avg_loss=0.0769 avg_acc=0.9801\n",
      "Batch 3000/3190 | avg_loss=0.0780 avg_acc=0.9799\n",
      "Batch 3000/3190 | avg_loss=0.0780 avg_acc=0.9799\n",
      "Epoch 6/12 | train_loss=0.0822 train_acc=0.9789 | val_loss=0.0556 val_acc=0.9855\n",
      "Epoch 6/12 | train_loss=0.0822 train_acc=0.9789 | val_loss=0.0556 val_acc=0.9855\n",
      "Batch 500/3190 | avg_loss=0.0799 avg_acc=0.9787\n",
      "Batch 500/3190 | avg_loss=0.0799 avg_acc=0.9787\n",
      "Batch 1000/3190 | avg_loss=0.0757 avg_acc=0.9809\n",
      "Batch 1000/3190 | avg_loss=0.0757 avg_acc=0.9809\n",
      "Batch 1500/3190 | avg_loss=0.0752 avg_acc=0.9811\n",
      "Batch 1500/3190 | avg_loss=0.0752 avg_acc=0.9811\n",
      "Batch 2000/3190 | avg_loss=0.0753 avg_acc=0.9797\n",
      "Batch 2000/3190 | avg_loss=0.0753 avg_acc=0.9797\n",
      "Batch 2500/3190 | avg_loss=0.0766 avg_acc=0.9797\n",
      "Batch 2500/3190 | avg_loss=0.0766 avg_acc=0.9797\n",
      "Batch 3000/3190 | avg_loss=0.0723 avg_acc=0.9817\n",
      "Batch 3000/3190 | avg_loss=0.0723 avg_acc=0.9817\n",
      "Epoch 7/12 | train_loss=0.0759 train_acc=0.9803 | val_loss=0.0515 val_acc=0.9867\n",
      "Epoch 7/12 | train_loss=0.0759 train_acc=0.9803 | val_loss=0.0515 val_acc=0.9867\n",
      "Batch 500/3190 | avg_loss=0.0730 avg_acc=0.9807\n",
      "Batch 500/3190 | avg_loss=0.0730 avg_acc=0.9807\n",
      "Batch 1000/3190 | avg_loss=0.0703 avg_acc=0.9818\n",
      "Batch 1000/3190 | avg_loss=0.0703 avg_acc=0.9818\n",
      "Batch 1500/3190 | avg_loss=0.0734 avg_acc=0.9815\n",
      "Batch 1500/3190 | avg_loss=0.0734 avg_acc=0.9815\n",
      "Batch 2000/3190 | avg_loss=0.0698 avg_acc=0.9818\n",
      "Batch 2000/3190 | avg_loss=0.0698 avg_acc=0.9818\n",
      "Batch 2500/3190 | avg_loss=0.0720 avg_acc=0.9817\n",
      "Batch 2500/3190 | avg_loss=0.0720 avg_acc=0.9817\n",
      "Batch 3000/3190 | avg_loss=0.0721 avg_acc=0.9815\n",
      "Batch 3000/3190 | avg_loss=0.0721 avg_acc=0.9815\n",
      "Epoch 8/12 | train_loss=0.0715 train_acc=0.9815 | val_loss=0.0504 val_acc=0.9867\n",
      "Epoch 8/12 | train_loss=0.0715 train_acc=0.9815 | val_loss=0.0504 val_acc=0.9867\n",
      "Batch 500/3190 | avg_loss=0.0666 avg_acc=0.9824\n",
      "Batch 500/3190 | avg_loss=0.0666 avg_acc=0.9824\n",
      "Batch 1000/3190 | avg_loss=0.0700 avg_acc=0.9828\n",
      "Batch 1000/3190 | avg_loss=0.0700 avg_acc=0.9828\n",
      "Batch 1500/3190 | avg_loss=0.0668 avg_acc=0.9829\n",
      "Batch 1500/3190 | avg_loss=0.0668 avg_acc=0.9829\n",
      "Batch 2000/3190 | avg_loss=0.0691 avg_acc=0.9819\n",
      "Batch 2000/3190 | avg_loss=0.0691 avg_acc=0.9819\n",
      "Batch 2500/3190 | avg_loss=0.0647 avg_acc=0.9828\n",
      "Batch 2500/3190 | avg_loss=0.0647 avg_acc=0.9828\n",
      "Batch 3000/3190 | avg_loss=0.0655 avg_acc=0.9825\n",
      "Batch 3000/3190 | avg_loss=0.0655 avg_acc=0.9825\n",
      "Epoch 9/12 | train_loss=0.0670 train_acc=0.9826 | val_loss=0.0479 val_acc=0.9873\n",
      "Epoch 9/12 | train_loss=0.0670 train_acc=0.9826 | val_loss=0.0479 val_acc=0.9873\n",
      "Batch 500/3190 | avg_loss=0.0689 avg_acc=0.9817\n",
      "Batch 500/3190 | avg_loss=0.0689 avg_acc=0.9817\n",
      "Batch 1000/3190 | avg_loss=0.0644 avg_acc=0.9825\n",
      "Batch 1000/3190 | avg_loss=0.0644 avg_acc=0.9825\n",
      "Batch 1500/3190 | avg_loss=0.0669 avg_acc=0.9823\n",
      "Batch 1500/3190 | avg_loss=0.0669 avg_acc=0.9823\n",
      "Batch 2000/3190 | avg_loss=0.0626 avg_acc=0.9838\n",
      "Batch 2000/3190 | avg_loss=0.0626 avg_acc=0.9838\n",
      "Batch 2500/3190 | avg_loss=0.0632 avg_acc=0.9835\n",
      "Batch 2500/3190 | avg_loss=0.0632 avg_acc=0.9835\n",
      "Batch 3000/3190 | avg_loss=0.0610 avg_acc=0.9843\n",
      "Batch 3000/3190 | avg_loss=0.0610 avg_acc=0.9843\n",
      "Epoch 10/12 | train_loss=0.0642 train_acc=0.9831 | val_loss=0.0450 val_acc=0.9880\n",
      "Epoch 10/12 | train_loss=0.0642 train_acc=0.9831 | val_loss=0.0450 val_acc=0.9880\n",
      "Batch 500/3190 | avg_loss=0.0593 avg_acc=0.9844\n",
      "Batch 500/3190 | avg_loss=0.0593 avg_acc=0.9844\n",
      "Batch 1000/3190 | avg_loss=0.0618 avg_acc=0.9840\n",
      "Batch 1000/3190 | avg_loss=0.0618 avg_acc=0.9840\n",
      "Batch 1500/3190 | avg_loss=0.0632 avg_acc=0.9837\n",
      "Batch 1500/3190 | avg_loss=0.0632 avg_acc=0.9837\n",
      "Batch 2000/3190 | avg_loss=0.0624 avg_acc=0.9840\n",
      "Batch 2000/3190 | avg_loss=0.0624 avg_acc=0.9840\n",
      "Batch 2500/3190 | avg_loss=0.0592 avg_acc=0.9840\n",
      "Batch 2500/3190 | avg_loss=0.0592 avg_acc=0.9840\n",
      "Batch 3000/3190 | avg_loss=0.0589 avg_acc=0.9842\n",
      "Batch 3000/3190 | avg_loss=0.0589 avg_acc=0.9842\n",
      "Epoch 11/12 | train_loss=0.0609 train_acc=0.9840 | val_loss=0.0431 val_acc=0.9886\n",
      "Epoch 11/12 | train_loss=0.0609 train_acc=0.9840 | val_loss=0.0431 val_acc=0.9886\n",
      "Batch 500/3190 | avg_loss=0.0616 avg_acc=0.9843\n",
      "Batch 500/3190 | avg_loss=0.0616 avg_acc=0.9843\n",
      "Batch 1000/3190 | avg_loss=0.0622 avg_acc=0.9837\n",
      "Batch 1000/3190 | avg_loss=0.0622 avg_acc=0.9837\n",
      "Batch 1500/3190 | avg_loss=0.0576 avg_acc=0.9845\n",
      "Batch 1500/3190 | avg_loss=0.0576 avg_acc=0.9845\n",
      "Batch 2000/3190 | avg_loss=0.0588 avg_acc=0.9841\n",
      "Batch 2000/3190 | avg_loss=0.0588 avg_acc=0.9841\n",
      "Batch 2500/3190 | avg_loss=0.0570 avg_acc=0.9849\n",
      "Batch 2500/3190 | avg_loss=0.0570 avg_acc=0.9849\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     50\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 51\u001b[0m     tr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# run validation inside epoch (after training pass)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     va \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion, log_interval)\u001b[0m\n\u001b[1;32m     11\u001b[0m window_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m window_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Support both dict-style batches (from our Dataset) and tuple batches\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     16\u001b[0m         xb \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandmarks\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Code/Fall25/hand-gestures-classifier/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m~/Code/Fall25/hand-gestures-classifier/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Code/Fall25/hand-gestures-classifier/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Code/Fall25/hand-gestures-classifier/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Code/Fall25/hand-gestures-classifier/data_loaders.py:166\u001b[0m, in \u001b[0;36mLandmarksDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    164\u001b[0m \tlm[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (lm[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m bx) \u001b[38;5;241m/\u001b[39m bw\n\u001b[1;32m    165\u001b[0m \tlm[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m (lm[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m by) \u001b[38;5;241m/\u001b[39m bh\n\u001b[0;32m--> 166\u001b[0m \t\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m \tlm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(lm)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Simple hyperparameter grid search (small grid for demo)\n",
    "results = []\n",
    "save_dir = os.path.join('..', os.getcwd(), 'training_outputs')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "param_grid = {\n",
    "    'lr': [3e-4, 1e-3],\n",
    "\n",
    "    'hidden_dims': [\n",
    "        (128, 64),\n",
    "        (256, 128),\n",
    "    ],\n",
    "\n",
    "    'dropout': [0.1, 0.3],\n",
    "\n",
    "    'batchnorm': [False, True],\n",
    "\n",
    "    'batch_size': [128, 256],\n",
    "\n",
    "    'max_epochs': [8, 12],\n",
    "}\n",
    "\n",
    "# Loss (kept separate for clarity)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# iterate grid\n",
    "# How often to print batch-level progress inside an epoch (set 0 to disable):\n",
    "log_interval = 500\n",
    "for lr, hidden_dims, dropout, batchnorm, batch_size, max_epochs in product(\n",
    "    param_grid['lr'], param_grid['hidden_dims'], param_grid['dropout'], param_grid['batchnorm'],\n",
    "    param_grid['batch_size'], param_grid['max_epochs']\n",
    "):\n",
    "    print('Running config:', {'lr': lr, 'hidden_dims': hidden_dims, 'dropout': dropout, 'batchnorm': batchnorm, 'batch_size': batch_size, 'max_epochs': max_epochs})\n",
    "\n",
    "    # build model and loaders\n",
    "    model = create_mlp_baseline(input_dim=input_dim, num_classes=num_classes, hidden_dims=hidden_dims, dropout=dropout, activation='relu', batchnorm=batchnorm)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # create DataLoaders directly (no helper)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state: dict[str, Any] | None = None\n",
    "\n",
    "    # history for this config (per-epoch)\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        prefix = f\"Config lr={lr} bs={batch_size} | Epoch {epoch}/{max_epochs}\"\n",
    "        tr = train_epoch(model, train_loader, optimizer, criterion, log_interval=log_interval)\n",
    "        # run validation inside epoch (after training pass)\n",
    "        va = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        # record history\n",
    "        history['train_loss'].append(tr['loss'])\n",
    "        history['train_acc'].append(tr['acc'])\n",
    "        history['val_loss'].append(va['loss'])\n",
    "        history['val_acc'].append(va['acc'])\n",
    "\n",
    "        # more detailed per-epoch message\n",
    "        print(f\"Epoch {epoch}/{max_epochs} | train_loss={tr['loss']:.4f} train_acc={tr['acc']:.4f} | val_loss={va['loss']:.4f} val_acc={va['acc']:.4f}\")\n",
    "\n",
    "        # save best model for this config\n",
    "        if va['acc'] > best_val_acc:\n",
    "            best_val_acc = va['acc']\n",
    "            best_state = {\n",
    "                'model_state': model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "\n",
    "    # persist best for this config\n",
    "    config_name = f\"mlp_lr{lr}_hd{hidden_dims[0]}-{hidden_dims[1]}_do{int(dropout*100)}_bn{int(batchnorm)}_bs{batch_size}_ep{max_epochs}\"\n",
    "    out_path = os.path.join(save_dir, config_name + '.pth')\n",
    "    if best_state is not None:\n",
    "        torch.save({'config': {'lr': lr, 'hidden_dims': hidden_dims, 'dropout': dropout, 'batchnorm': batchnorm, 'batch_size': batch_size, 'max_epochs': max_epochs}, 'best_val_acc': best_val_acc, 'state': best_state}, out_path)\n",
    "\n",
    "    results.append({'config': {'lr': lr, 'hidden_dims': hidden_dims, 'dropout': dropout, 'batchnorm': batchnorm, 'batch_size': batch_size, 'max_epochs': max_epochs}, 'best_val_acc': best_val_acc, 'path': out_path, 'history': history})\n",
    "\n",
    "# summarize results\n",
    "results_sorted = sorted(results, key=lambda r: r['best_val_acc'], reverse=True)\n",
    "print('Top results:')\n",
    "for r in results_sorted[:5]:\n",
    "    print(r['best_val_acc'], r['config'], r['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and do a quick inference check\n",
    "best = results_sorted[0]\n",
    "print('Best config:', best['config'], 'val_acc=', best['best_val_acc'])\n",
    "ckpt = torch.load(best['path'], map_location=device)\n",
    "cfg = ckpt['config']\n",
    "model = create_mlp_baseline(input_dim=input_dim, num_classes=num_classes, hidden_dims=tuple(cfg['hidden_dims']), dropout=cfg['dropout'], activation='relu', batchnorm=cfg['batchnorm'])\n",
    "model.load_state_dict(ckpt['state']['model_state'])\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# run inference on a small batch\n",
    "batch = next(iter(val_loader))\n",
    "if isinstance(batch, dict):\n",
    "    xb = batch['landmarks']\n",
    "    yb = batch['label']\n",
    "else:\n",
    "    xb, yb = batch\n",
    "with torch.no_grad():\n",
    "    logits = model(xb.to(device))\n",
    "    preds = logits.argmax(dim=1).cpu()\n",
    "\n",
    "print('Sample preds:', preds[:10].tolist())\n",
    "print('Sample labels:', yb[:10].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acfc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/validation history for the best config\n",
    "import matplotlib.pyplot as plt\n",
    "best = results_sorted[0]\n",
    "hist = best.get('history')\n",
    "if hist is None:\n",
    "    print('No history available for best config')\n",
    "else:\n",
    "    epochs = list(range(1, len(hist['train_loss']) + 1))\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, hist['train_loss'], label='train_loss')\n",
    "    plt.plot(epochs, hist['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, hist['train_acc'], label='train_acc')\n",
    "    plt.plot(epochs, hist['val_acc'], label='val_acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hand-gestures-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
